{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just to know how much time will it take to run this entire ipython notebook \n",
    "from datetime import datetime\n",
    "# globalstart = datetime.now()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('nbagg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import os\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is present in your pwd, getting it from disk....\n",
      "DONE..\n",
      "0:00:07.699237\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "if os.path.isfile('train_sparse_matrix.npz'):\n",
    "    print(\"It is present in your pwd, getting it from disk....\")\n",
    "    # just get it from the disk instead of computing it\n",
    "    train_sparse_matrix = sparse.load_npz('train_sparse_matrix.npz')\n",
    "    print(\"DONE..\")\n",
    "else: \n",
    "    print(\"We are creating sparse_matrix from the dataframe..\")\n",
    "    # create sparse_matrix and store it for after usage.\n",
    "    # csr_matrix(data_values, (row_index, col_index), shape_of_matrix)\n",
    "    # It should be in such a way that, MATRIX[row, col] = data\n",
    "    train_sparse_matrix = sparse.csr_matrix((train_df.rating.values, (train_df.user.values,\n",
    "                                               train_df.movie.values)),)\n",
    "    \n",
    "    print('Done. It\\'s shape is : (user, movie) : ',train_sparse_matrix.shape)\n",
    "    print('Saving it into disk for furthur usage..')\n",
    "    # save it into disk\n",
    "    sparse.save_npz(\"train_sparse_matrix.npz\", train_sparse_matrix)\n",
    "    print('Done..\\n')\n",
    "\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is present in your pwd, getting it from disk....\n",
      "DONE..\n",
      "0:00:02.108804\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "if os.path.isfile('test_sparse_matrix.npz'):\n",
    "    print(\"It is present in your pwd, getting it from disk....\")\n",
    "    # just get it from the disk instead of computing it\n",
    "    test_sparse_matrix = sparse.load_npz('test_sparse_matrix.npz')\n",
    "    print(\"DONE..\")\n",
    "else: \n",
    "    print(\"We are creating sparse_matrix from the dataframe..\")\n",
    "    # create sparse_matrix and store it for after usage.\n",
    "    # csr_matrix(data_values, (row_index, col_index), shape_of_matrix)\n",
    "    # It should be in such a way that, MATRIX[row, col] = data\n",
    "    test_sparse_matrix = sparse.csr_matrix((test_df.rating.values, (test_df.user.values,\n",
    "                                               test_df.movie.values)))\n",
    "    \n",
    "    print('Done. It\\'s shape is : (user, movie) : ',test_sparse_matrix.shape)\n",
    "    print('Saving it into disk for furthur usage..')\n",
    "    # save it into disk\n",
    "    sparse.save_npz(\"test_sparse_matrix.npz\", test_sparse_matrix)\n",
    "    print('Done..\\n')\n",
    "    \n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_sparse_matrix(sparse_matrix, no_users, no_movies, path, verbose = True):\n",
    "    \"\"\"\n",
    "        It will get it from the ''path'' if it is present  or It will create \n",
    "        and store the sampled sparse matrix in the path specified.\n",
    "    \"\"\"\n",
    "\n",
    "    # get (row, col) and (rating) tuple from sparse_matrix...\n",
    "    row_ind, col_ind, ratings = sparse.find(sparse_matrix)\n",
    "    users = np.unique(row_ind)\n",
    "    movies = np.unique(col_ind)\n",
    "\n",
    "    print(\"Original Matrix : (users, movies) -- ({} {})\".format(len(users), len(movies)))\n",
    "    print(\"Original Matrix : Ratings -- {}\\n\".format(len(ratings)))\n",
    "\n",
    "    # It just to make sure to get same sample everytime we run this program..\n",
    "    # and pick without replacement....\n",
    "    np.random.seed(15)\n",
    "    sample_users = np.random.choice(users, no_users, replace=False)\n",
    "    sample_movies = np.random.choice(movies, no_movies, replace=False)\n",
    "    # get the boolean mask or these sampled_items in originl row/col_inds..\n",
    "    mask = np.logical_and( np.isin(row_ind, sample_users),\n",
    "                      np.isin(col_ind, sample_movies) )\n",
    "    \n",
    "    sample_sparse_matrix = sparse.csr_matrix((ratings[mask], (row_ind[mask], col_ind[mask])),\n",
    "                                             shape=(max(sample_users)+1, max(sample_movies)+1))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Sampled Matrix : (users, movies) -- ({} {})\".format(len(sample_users), len(sample_movies)))\n",
    "        print(\"Sampled Matrix : Ratings --\", format(ratings[mask].shape[0]))\n",
    "\n",
    "    print('Saving it into disk for furthur usage..')\n",
    "    # save it into disk\n",
    "    sparse.save_npz(path, sample_sparse_matrix)\n",
    "    if verbose:\n",
    "            print('Done..\\n')\n",
    "    \n",
    "    return sample_sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is present in your pwd, getting it from disk....\n",
      "DONE..\n",
      "0:00:00.249600\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "path = \"sample_train_sparse_matrix.npz\"\n",
    "if os.path.isfile(path):\n",
    "    print(\"It is present in your pwd, getting it from disk....\")\n",
    "    # just get it from the disk instead of computing it\n",
    "    sample_train_sparse_matrix = sparse.load_npz(path)\n",
    "    print(\"DONE..\")\n",
    "else: \n",
    "    # get 10k users and 1k movies from available data \n",
    "    sample_train_sparse_matrix = get_sample_sparse_matrix(train_sparse_matrix, no_users=25000, no_movies=3000,\n",
    "                                             path = path)\n",
    "\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is present in your pwd, getting it from disk....\n",
      "DONE..\n",
      "0:00:00.199865\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "path = \"sample_test_sparse_matrix.npz\"\n",
    "if os.path.isfile(path):\n",
    "    print(\"It is present in your pwd, getting it from disk....\")\n",
    "    # just get it from the disk instead of computing it\n",
    "    sample_test_sparse_matrix = sparse.load_npz(path)\n",
    "    print(\"DONE..\")\n",
    "else:\n",
    "    # get 5k users and 500 movies from available data \n",
    "    sample_test_sparse_matrix = get_sample_sparse_matrix(test_sparse_matrix, no_users=25000, no_movies=3000,\n",
    "                                                 path = path)\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the user averages in dictionary (key: user_id/movie_id, value: avg rating)\n",
    "\n",
    "def get_average_ratings(sparse_matrix, of_users):\n",
    "    \n",
    "    # average ratings of user/axes\n",
    "    ax = 1 if of_users else 0 # 1 - User axes,0 - Movie axes\n",
    "\n",
    "    # \".A1\" is for converting Column_Matrix to 1-D numpy array\n",
    "    # axis = 0 means summing in columnwise, axis = 1 means summing in rowise\n",
    "    sum_of_ratings = sparse_matrix.sum(axis=ax).A1\n",
    "    # Boolean matrix of ratings ( whether a user rated that movie or not)\n",
    "    is_rated = sparse_matrix!=0\n",
    "    # no of ratings that each user OR movie..\n",
    "    no_of_ratings = is_rated.sum(axis=ax).A1\n",
    "    \n",
    "    # max_user  and max_movie ids in sparse matrix \n",
    "    u,m = sparse_matrix.shape\n",
    "    # creae a dictonary of users and their average ratigns..\n",
    "    average_ratings = { i : sum_of_ratings[i]/no_of_ratings[i]\n",
    "                                 for i in range(u if of_users else m) \n",
    "                                    if no_of_ratings[i] !=0}\n",
    "\n",
    "    # return that dictionary of average ratings\n",
    "    return average_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average rating of user 1515220 : 3.923076923076923\n",
      "\n",
      " AVerage rating of movie 15153 : 2.752\n"
     ]
    }
   ],
   "source": [
    "sample_train_averages = dict()\n",
    "# get the global average of ratings in our train set.\n",
    "global_average = sample_train_sparse_matrix.sum()/sample_train_sparse_matrix.count_nonzero()\n",
    "sample_train_averages['global'] = global_average\n",
    "sample_train_averages\n",
    "sample_train_averages['user'] = get_average_ratings(sample_train_sparse_matrix, of_users=True)\n",
    "print('\\nAverage rating of user 1515220 :',sample_train_averages['user'][1515220])\n",
    "sample_train_averages['movie'] =  get_average_ratings(sample_train_sparse_matrix, of_users=False)\n",
    "print('\\n AVerage rating of movie 15153 :',sample_train_averages['movie'][15153])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " No of ratings in Our Sampled train matrix is : 856986\n",
      "\n",
      "\n",
      " No of ratings in Our Sampled test  matrix is : 261693\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n No of ratings in Our Sampled train matrix is : {}\\n'.format(sample_train_sparse_matrix.count_nonzero()))\n",
    "print('\\n No of ratings in Our Sampled test  matrix is : {}\\n'.format(sample_test_sparse_matrix.count_nonzero()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get users, movies and ratings from our samples train sparse matrix\n",
    "sample_train_users, sample_train_movies, sample_train_ratings = sparse.find(sample_train_sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing 856986 tuples for the dataset..\n",
      "\n",
      "Done for 10000 rows----- 1:00:16.111474\n",
      "Done for 20000 rows----- 2:01:24.226434\n",
      "Done for 30000 rows----- 3:01:59.348857\n",
      "Done for 40000 rows----- 4:02:20.276774\n",
      "Done for 50000 rows----- 5:02:42.899280\n",
      "Done for 60000 rows----- 6:03:06.157685\n",
      "Done for 70000 rows----- 7:03:24.127819\n",
      "Done for 80000 rows----- 8:03:45.400375\n",
      "Done for 90000 rows----- 9:04:05.371160\n",
      "Done for 100000 rows----- 10:04:28.935813\n",
      "Done for 110000 rows----- 11:04:51.049385\n",
      "Done for 120000 rows----- 12:05:19.705289\n",
      "Done for 130000 rows----- 13:05:37.918816\n",
      "Done for 140000 rows----- 14:06:01.346077\n",
      "Done for 150000 rows----- 15:06:30.446240\n",
      "Done for 160000 rows----- 16:06:45.872334\n",
      "Done for 170000 rows----- 17:07:11.366866\n",
      "Done for 180000 rows----- 18:07:30.243634\n",
      "Done for 190000 rows----- 19:08:11.344146\n",
      "Done for 200000 rows----- 20:08:48.670657\n",
      "Done for 210000 rows----- 21:09:31.188807\n",
      "Done for 220000 rows----- 22:10:09.857719\n",
      "Done for 230000 rows----- 23:10:46.165342\n",
      "Done for 240000 rows----- 1 day, 0:11:30.366981\n",
      "Done for 250000 rows----- 1 day, 1:12:14.193496\n",
      "Done for 260000 rows----- 1 day, 2:13:05.882457\n",
      "Done for 270000 rows----- 1 day, 3:13:28.502089\n",
      "Done for 280000 rows----- 1 day, 4:13:58.029943\n",
      "Done for 290000 rows----- 1 day, 5:14:24.478155\n",
      "Done for 300000 rows----- 1 day, 6:14:53.011221\n",
      "Done for 310000 rows----- 1 day, 7:15:11.866540\n",
      "Done for 320000 rows----- 1 day, 8:15:32.730358\n",
      "Done for 330000 rows----- 1 day, 9:15:57.712568\n",
      "Done for 340000 rows----- 1 day, 10:16:17.318660\n",
      "Done for 350000 rows----- 1 day, 11:16:32.864872\n",
      "Done for 360000 rows----- 1 day, 12:16:53.435769\n",
      "Done for 370000 rows----- 1 day, 13:17:29.276743\n",
      "Done for 380000 rows----- 1 day, 14:17:56.026225\n",
      "Done for 390000 rows----- 1 day, 15:18:21.829737\n",
      "Done for 400000 rows----- 1 day, 16:18:46.268872\n",
      "Done for 410000 rows----- 1 day, 17:19:06.259193\n",
      "Done for 420000 rows----- 1 day, 18:19:33.779168\n",
      "Done for 430000 rows----- 1 day, 19:20:09.704944\n",
      "Done for 440000 rows----- 1 day, 20:20:37.345387\n",
      "Done for 450000 rows----- 1 day, 21:21:03.881119\n",
      "Done for 460000 rows----- 1 day, 22:21:32.945024\n",
      "Done for 470000 rows----- 1 day, 23:22:22.019602\n",
      "Done for 480000 rows----- 2 days, 0:22:48.708510\n",
      "Done for 490000 rows----- 2 days, 1:23:17.809891\n",
      "Done for 500000 rows----- 2 days, 2:23:48.424257\n",
      "Done for 510000 rows----- 2 days, 3:24:13.532205\n",
      "Done for 520000 rows----- 2 days, 4:24:49.609085\n",
      "Done for 530000 rows----- 2 days, 5:25:17.058869\n",
      "Done for 540000 rows----- 2 days, 6:25:42.799320\n",
      "Done for 550000 rows----- 2 days, 7:26:02.522845\n",
      "Done for 560000 rows----- 2 days, 8:26:20.321404\n",
      "Done for 570000 rows----- 2 days, 9:26:43.312441\n",
      "Done for 580000 rows----- 2 days, 10:26:58.919377\n",
      "Done for 590000 rows----- 2 days, 11:27:17.262258\n",
      "Done for 600000 rows----- 2 days, 12:27:44.451688\n",
      "Done for 610000 rows----- 2 days, 13:28:04.333995\n",
      "Done for 620000 rows----- 2 days, 14:28:25.666998\n",
      "Done for 630000 rows----- 2 days, 15:30:37.438966\n",
      "Done for 640000 rows----- 2 days, 16:30:49.786729\n",
      "Done for 650000 rows----- 2 days, 17:30:59.133406\n",
      "Done for 660000 rows----- 2 days, 18:31:23.885352\n",
      "Done for 670000 rows----- 2 days, 19:31:49.587578\n",
      "Done for 680000 rows----- 2 days, 20:32:13.326090\n",
      "Done for 690000 rows----- 2 days, 21:32:39.499279\n",
      "Done for 700000 rows----- 2 days, 22:33:00.565638\n",
      "Done for 710000 rows----- 2 days, 23:33:18.891881\n",
      "Done for 720000 rows----- 3 days, 0:34:13.416294\n",
      "Done for 730000 rows----- 3 days, 1:45:26.643599\n",
      "Done for 740000 rows----- 3 days, 2:46:52.884561\n",
      "Done for 750000 rows----- 3 days, 3:47:51.848771\n",
      "Done for 760000 rows----- 3 days, 4:48:26.217192\n",
      "Done for 770000 rows----- 3 days, 5:49:07.982985\n",
      "Done for 780000 rows----- 3 days, 6:49:46.805273\n",
      "Done for 790000 rows----- 3 days, 7:50:22.033274\n",
      "Done for 800000 rows----- 3 days, 8:50:54.612062\n",
      "Done for 810000 rows----- 3 days, 9:51:32.268196\n",
      "Done for 820000 rows----- 3 days, 10:52:10.131807\n",
      "Done for 830000 rows----- 3 days, 11:52:51.480238\n",
      "Done for 840000 rows----- 3 days, 12:53:22.484378\n",
      "Done for 850000 rows----- 3 days, 13:53:44.850343\n",
      "3 days, 14:36:01.530250\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "# It took me almost 10 hours to prepare this train dataset.#\n",
    "############################################################\n",
    "start = datetime.now()\n",
    "if os.path.isfile('reg_train.csv'):\n",
    "    print(\"File already exists you don't have to prepare again...\" )\n",
    "else:\n",
    "    print('preparing {} tuples for the dataset..\\n'.format(len(sample_train_ratings)))\n",
    "    with open('reg_train.csv', mode='w') as reg_data_file:\n",
    "        count = 0\n",
    "        for (user, movie, rating)  in zip(sample_train_users, sample_train_movies, sample_train_ratings):\n",
    "            st = datetime.now()\n",
    "#             print(user, movie)    \n",
    "            #--------------------- Ratings of \"movie\" by similar users of \"user\" ---------------------\n",
    "            # compute the similar Users of the \"user\"        \n",
    "            user_sim = cosine_similarity(sample_train_sparse_matrix[user], sample_train_sparse_matrix).ravel()\n",
    "            top_sim_users = user_sim.argsort()[::-1][1:] # we are ignoring 'The User' from its similar users.\n",
    "            # get the ratings of most similar users for this movie\n",
    "            top_ratings = sample_train_sparse_matrix[top_sim_users, movie].toarray().ravel()\n",
    "            # we will make it's length \"5\" by adding movie averages to .\n",
    "            top_sim_users_ratings = list(top_ratings[top_ratings != 0][:5])\n",
    "            top_sim_users_ratings.extend([sample_train_averages['movie'][movie]]*(5 - len(top_sim_users_ratings)))\n",
    "        #     print(top_sim_users_ratings, end=\" \")    \n",
    "\n",
    "\n",
    "            #--------------------- Ratings by \"user\"  to similar movies of \"movie\" ---------------------\n",
    "            # compute the similar movies of the \"movie\"        \n",
    "            movie_sim = cosine_similarity(sample_train_sparse_matrix[:,movie].T, sample_train_sparse_matrix.T).ravel()\n",
    "            top_sim_movies = movie_sim.argsort()[::-1][1:] # we are ignoring 'The User' from its similar users.\n",
    "            # get the ratings of most similar movie rated by this user..\n",
    "            top_ratings = sample_train_sparse_matrix[user, top_sim_movies].toarray().ravel()\n",
    "            # we will make it's length \"5\" by adding user averages to.\n",
    "            top_sim_movies_ratings = list(top_ratings[top_ratings != 0][:5])\n",
    "            top_sim_movies_ratings.extend([sample_train_averages['user'][user]]*(5-len(top_sim_movies_ratings))) \n",
    "        #     print(top_sim_movies_ratings, end=\" : -- \")\n",
    "\n",
    "            #-----------------prepare the row to be stores in a file-----------------#\n",
    "            row = list()\n",
    "            row.append(user)\n",
    "            row.append(movie)\n",
    "            # Now add the other features to this data...\n",
    "            row.append(sample_train_averages['global']) # first feature\n",
    "            # next 5 features are similar_users \"movie\" ratings\n",
    "            row.extend(top_sim_users_ratings)\n",
    "            # next 5 features are \"user\" ratings for similar_movies\n",
    "            row.extend(top_sim_movies_ratings)\n",
    "            # Avg_user rating\n",
    "            row.append(sample_train_averages['user'][user])\n",
    "            # Avg_movie rating\n",
    "            row.append(sample_train_averages['movie'][movie])\n",
    "\n",
    "            # finalley, The actual Rating of this user-movie pair...\n",
    "            row.append(rating)\n",
    "            count = count + 1\n",
    "\n",
    "            # add rows to the file opened..\n",
    "            reg_data_file.write(','.join(map(str, row)))\n",
    "            reg_data_file.write('\\n')        \n",
    "            if (count)%10000 == 0:\n",
    "                # print(','.join(map(str, row)))\n",
    "                print(\"Done for {} rows----- {}\".format(count, datetime.now() - start))\n",
    "\n",
    "\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get users, movies and ratings from our samples train sparse matrix\n",
    "sample_test_users, sample_test_movies, sample_test_ratings = sparse.find(sample_test_sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing 261693 tuples for the dataset..\n",
      "\n",
      "Done for 10000 rows----- 0:59:25.521386\n",
      "Done for 20000 rows----- 1:57:42.951883\n",
      "Done for 30000 rows----- 2:55:54.486925\n",
      "Done for 40000 rows----- 3:54:18.535002\n",
      "Done for 50000 rows----- 4:52:41.195504\n",
      "Done for 60000 rows----- 5:51:03.183363\n",
      "Done for 70000 rows----- 6:49:34.266370\n",
      "Done for 80000 rows----- 7:48:00.188456\n",
      "Done for 90000 rows----- 8:46:30.922874\n",
      "Done for 100000 rows----- 9:44:46.557478\n",
      "Done for 110000 rows----- 10:43:07.532189\n",
      "Done for 120000 rows----- 11:41:28.590011\n",
      "Done for 130000 rows----- 12:39:47.401122\n",
      "Done for 140000 rows----- 13:38:03.682860\n",
      "Done for 150000 rows----- 14:36:26.280760\n",
      "Done for 160000 rows----- 15:34:45.855514\n",
      "Done for 170000 rows----- 16:33:02.891295\n",
      "Done for 180000 rows----- 17:31:19.850502\n",
      "Done for 190000 rows----- 18:29:38.867130\n",
      "Done for 200000 rows----- 19:27:57.149108\n",
      "Done for 210000 rows----- 20:26:19.723569\n",
      "Done for 220000 rows----- 21:24:32.582090\n",
      "Done for 230000 rows----- 22:22:47.428998\n",
      "Done for 240000 rows----- 23:21:11.158134\n",
      "Done for 250000 rows----- 1 day, 0:19:30.153314\n",
      "Done for 260000 rows----- 1 day, 1:17:39.549056\n",
      " 1 day, 1:27:26.751587\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "if os.path.isfile('reg_test.csv'):\n",
    "    print(\"It is already created...\")\n",
    "else:\n",
    "\n",
    "    print('preparing {} tuples for the dataset..\\n'.format(len(sample_test_ratings)))\n",
    "    with open('reg_test.csv', mode='w') as reg_data_file:\n",
    "        count = 0 \n",
    "        for (user, movie, rating)  in zip(sample_test_users, sample_test_movies, sample_test_ratings):\n",
    "            st = datetime.now()\n",
    "\n",
    "        #--------------------- Ratings of \"movie\" by similar users of \"user\" ---------------------\n",
    "            #print(user, movie)\n",
    "            try:\n",
    "                # compute the similar Users of the \"user\"        \n",
    "                user_sim = cosine_similarity(sample_train_sparse_matrix[user], sample_train_sparse_matrix).ravel()\n",
    "                top_sim_users = user_sim.argsort()[::-1][1:] # we are ignoring 'The User' from its similar users.\n",
    "                # get the ratings of most similar users for this movie\n",
    "                top_ratings = sample_train_sparse_matrix[top_sim_users, movie].toarray().ravel()\n",
    "                # we will make it's length \"5\" by adding movie averages to .\n",
    "                top_sim_users_ratings = list(top_ratings[top_ratings != 0][:5])\n",
    "                top_sim_users_ratings.extend([sample_train_averages['movie'][movie]]*(5 - len(top_sim_users_ratings)))\n",
    "                # print(top_sim_users_ratings, end=\"--\")\n",
    "\n",
    "            except (IndexError, KeyError):\n",
    "                # It is a new User or new Movie or there are no ratings for given user for top similar movies...\n",
    "                ########## Cold STart Problem ##########\n",
    "                top_sim_users_ratings.extend([sample_train_averages['global']]*(5 - len(top_sim_users_ratings)))\n",
    "                #print(top_sim_users_ratings)\n",
    "            except:\n",
    "                print(user, movie)\n",
    "                # we just want KeyErrors to be resolved. Not every Exception...\n",
    "                raise\n",
    "\n",
    "\n",
    "\n",
    "            #--------------------- Ratings by \"user\"  to similar movies of \"movie\" ---------------------\n",
    "            try:\n",
    "                # compute the similar movies of the \"movie\"        \n",
    "                movie_sim = cosine_similarity(sample_train_sparse_matrix[:,movie].T, sample_train_sparse_matrix.T).ravel()\n",
    "                top_sim_movies = movie_sim.argsort()[::-1][1:] # we are ignoring 'The User' from its similar users.\n",
    "                # get the ratings of most similar movie rated by this user..\n",
    "                top_ratings = sample_train_sparse_matrix[user, top_sim_movies].toarray().ravel()\n",
    "                # we will make it's length \"5\" by adding user averages to.\n",
    "                top_sim_movies_ratings = list(top_ratings[top_ratings != 0][:5])\n",
    "                top_sim_movies_ratings.extend([sample_train_averages['user'][user]]*(5-len(top_sim_movies_ratings))) \n",
    "                #print(top_sim_movies_ratings)\n",
    "            except (IndexError, KeyError):\n",
    "                #print(top_sim_movies_ratings, end=\" : -- \")\n",
    "                top_sim_movies_ratings.extend([sample_train_averages['global']]*(5-len(top_sim_movies_ratings)))\n",
    "                #print(top_sim_movies_ratings)\n",
    "            except :\n",
    "                raise\n",
    "\n",
    "            #-----------------prepare the row to be stores in a file-----------------#\n",
    "            row = list()\n",
    "            # add usser and movie name first\n",
    "            row.append(user)\n",
    "            row.append(movie)\n",
    "            row.append(sample_train_averages['global']) # first feature\n",
    "            #print(row)\n",
    "            # next 5 features are similar_users \"movie\" ratings\n",
    "            row.extend(top_sim_users_ratings)\n",
    "            #print(row)\n",
    "            # next 5 features are \"user\" ratings for similar_movies\n",
    "            row.extend(top_sim_movies_ratings)\n",
    "            #print(row)\n",
    "            # Avg_user rating\n",
    "            try:\n",
    "                row.append(sample_train_averages['user'][user])\n",
    "            except KeyError:\n",
    "                row.append(sample_train_averages['global'])\n",
    "            except:\n",
    "                raise\n",
    "            #print(row)\n",
    "            # Avg_movie rating\n",
    "            try:\n",
    "                row.append(sample_train_averages['movie'][movie])\n",
    "            except KeyError:\n",
    "                row.append(sample_train_averages['global'])\n",
    "            except:\n",
    "                raise\n",
    "            #print(row)\n",
    "            # finalley, The actual Rating of this user-movie pair...\n",
    "            row.append(rating)\n",
    "            #print(row)\n",
    "            count = count + 1\n",
    "\n",
    "            # add rows to the file opened..\n",
    "            reg_data_file.write(','.join(map(str, row)))\n",
    "            #print(','.join(map(str, row)))\n",
    "            reg_data_file.write('\\n')        \n",
    "            if (count)%10000 == 0:\n",
    "                #print(','.join(map(str, row)))\n",
    "                print(\"Done for {} rows----- {}\".format(count, datetime.now() - start))\n",
    "\n",
    "print(\"\",datetime.now() - start)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
